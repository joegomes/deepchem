{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong in DeepChem with A3C\n",
    "This notebook demonstrates using reinforcement learning to train an agent to play Pong.\n",
    "\n",
    "The first step is to create an `Environment` that implements this task.  Fortunately,\n",
    "OpenAI Gym already provides an implementation of Pong (and many other tasks appropriate\n",
    "for reinforcement learning).  DeepChem's `GymEnvironment` class provides an easy way to\n",
    "use environments from OpenAI Gym.  We could just use it directly, but in this case we\n",
    "subclass it and preprocess the screen image a little bit to make learning easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "\n",
    "class PongEnv(dc.rl.GymEnvironment):\n",
    "  def __init__(self):\n",
    "    super(PongEnv, self).__init__('Pong-v0')\n",
    "    self._state_shape = (80, 80)\n",
    "  \n",
    "  @property\n",
    "  def state(self):\n",
    "    # Crop everything outside the play area, reduce the image size,\n",
    "    # and convert it to black and white.\n",
    "    cropped = np.array(self._state)[34:194, :, :]\n",
    "    reduced = cropped[0:-1:2, 0:-1:2]\n",
    "    grayscale = np.sum(reduced, axis=2)\n",
    "    bw = np.zeros(grayscale.shape)\n",
    "    bw[grayscale != 233] = 1\n",
    "    return bw\n",
    "\n",
    "  def __deepcopy__(self, memo):\n",
    "    return PongEnv()\n",
    "\n",
    "env = PongEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a network to implement the policy.  We begin with two convolutional layers to process\n",
    "the image.  That is followed by a dense (fully connected) layer to provide plenty of capacity for game\n",
    "logic.  We also add a small Gated Recurrent Unit.  That gives the network a little bit of memory, so\n",
    "it can keep track of which way the ball is moving.\n",
    "\n",
    "We concatenate the dense and GRU outputs together, and use them as inputs to two final layers that serve as the\n",
    "network's outputs.  One computes the action probabilities, and the other computes an estimate of the\n",
    "state value function.\n",
    "\n",
    "We also provide an input for the initial state of the GRU, and returned its final state at the end.  This is required by the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Concatenate, Conv2D, Dense, Flatten, GRU, Reshape\n",
    "\n",
    "class PongPolicy(dc.rl.Policy):\n",
    "    def __init__(self):\n",
    "        super(PongPolicy, self).__init__(['action_prob', 'value', 'rnn_state'], [np.zeros(16)])\n",
    "\n",
    "    def create_model(self, **kwargs):\n",
    "        state = Input(shape=(80, 80))\n",
    "        rnn_state = Input(shape=(16,))\n",
    "        conv1 = Conv2D(16, kernel_size=8, strides=4, activation=tf.nn.relu)(Reshape((80, 80, 1))(state))\n",
    "        conv2 = Conv2D(32, kernel_size=4, strides=2, activation=tf.nn.relu)(conv1)\n",
    "        dense = Dense(256, activation=tf.nn.relu)(Flatten()(conv2))\n",
    "        gru, rnn_final_state = GRU(16, return_state=True, return_sequences=True)(\n",
    "            Reshape((-1, 256))(dense), initial_state=rnn_state)\n",
    "        concat = Concatenate()([dense, Reshape((16,))(gru)])\n",
    "        action_prob = Dense(env.n_actions, activation=tf.nn.softmax)(concat)\n",
    "        value = Dense(1)(concat)\n",
    "        return tf.keras.Model(inputs=[state, rnn_state], outputs=[action_prob, value, rnn_final_state])\n",
    "\n",
    "policy = PongPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will optimize the policy using the Asynchronous Advantage Actor Critic (A3C) algorithm.  There are lots of hyperparameters we could specify at this point, but the default values for most of them work well on this problem.  The only one we need to customize is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepchem.models.optimizers import Adam\n",
    "a3c = dc.rl.A3C(env, policy, model_dir='model', optimizer=Adam(learning_rate=0.0002))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize for as long as you have patience to.  By 1 million steps you should see clear signs of learning.  Around 3 million steps it should start to occasionally beat the game's built in AI.  By 7 million steps it should be winning almost every time.  Running on my laptop, training takes about 20 minutes for every million steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to train as many steps as you have patience for.\n",
    "a3c.fit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's watch it play and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "while not env.terminated:\n",
    "    env.env.render()\n",
    "    env.step(a3c.select_action(env.state))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
